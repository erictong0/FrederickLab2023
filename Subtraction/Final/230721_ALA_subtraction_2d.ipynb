{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erictong0/FrederickLab2023/blob/main/Subtraction/Final/230721_ALA_subtraction_2d.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bio --upgrade"
      ],
      "metadata": {
        "id": "qP8ucijHcqnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from posixpath import join\n",
        "import scipy as sp\n",
        "import sys\n",
        "import glob\n",
        "import math\n",
        "import pandas as pd\n",
        "import argparse\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as pltticker\n",
        "import csv\n",
        "import re\n",
        "import traceback\n",
        "from matplotlib import colors\n",
        "import matplotlib.cm as cm\n",
        "from statistics import mean\n",
        "from Bio.Data.IUPACData import protein_letters_1to3 as one2three"
      ],
      "metadata": {
        "id": "blO9oOOUUX1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQT8Emg03yKX"
      },
      "outputs": [],
      "source": [
        "\n",
        "from numpy import savetxt\n",
        "def main(PDB_file, exp_file, output_file_path, aa):\n",
        "  aa1 = aa\n",
        "  data = []\n",
        "  with open(PDB_file, mode=\"r\") as infile: #open data file and read contents\n",
        "    reader = csv.reader(infile, delimiter=\"\\n\")\n",
        "    for row in reader:\n",
        "      if len(row) == 1:\n",
        "        data.append(re.split(r'\\s+', row[0])[2:])\n",
        "      else:\n",
        "        data.append(row)\n",
        "  def get_row_floats(data, row):\n",
        "    return list(map(lambda x: float(x[row]), data))\n",
        "    #print(f'Data: {data}\\n\\n')\n",
        "\n",
        "  phi = get_row_floats(data[1:], 0) #def z as CO shift\n",
        "  psi = get_row_floats(data[1:], 1) #def w as N shift\n",
        "  x = get_row_floats(data[1:], 3)\n",
        "  y = get_row_floats(data[1:], 2) ######Switch\n",
        "  z = get_row_floats(data[1:], 4)\n",
        "  w = get_row_floats(data[1:], 5)\n",
        "  j = []\n",
        "  j = y\n",
        "  data = []\n",
        "  count = 0\n",
        "\n",
        "  with open(exp_file, mode=\"r\") as infile: #open data file and read contents\n",
        "    chunk_size = 1000\n",
        "    while True:\n",
        "        chunk = infile.readlines(chunk_size)\n",
        "        if not chunk:\n",
        "          break\n",
        "        for line in chunk:\n",
        "          reader = csv.reader(infile)\n",
        "          for row in reader:\n",
        "            count += 1\n",
        "            if len(row) == 1:\n",
        "              data.append(re.split(r'\\s+', row[0])[1:])\n",
        "            else:\n",
        "              data.append(row)\n",
        "\n",
        "  def get_row_floats(data, row):\n",
        "    return list(map(lambda x: float(x[row]), data))\n",
        "\n",
        "\n",
        "  ca_exp = get_row_floats(data, 0) #def x as Ca shift\n",
        "  inadequate = get_row_floats(data, 1)\n",
        "  cb_exp = get_row_floats(data, 1) #def y as Cb shift\n",
        "  intensity = get_row_floats(data, 2)\n",
        "\n",
        "\n",
        "  for i in range(len(inadequate)):\n",
        "    inadequate[i-1] = inadequate[i-1]-1\n",
        "  min = np.min(intensity)\n",
        "  for i in range(len(intensity)):\n",
        "    intensity[i-1] = intensity[i-1]-min\n",
        "\n",
        "  mask = np.logical_and(np.array(inadequate) >= 29, np.array(inadequate) <= 37)\n",
        "  mask = np.logical_and(mask, np.array(ca_exp) >= 54)\n",
        "  mask = np.logical_and(mask, np.array(ca_exp) <= 71)\n",
        "  #mask = np.logical_and(mask, np.array(intensity) >= 0)\n",
        "\n",
        "  #mask = np.logical_and(np.array(inadequate) >= 105, np.array(inadequate) <= 139)\n",
        "  #mask = np.logical_and(mask, np.array(ca_exp) >= 50)\n",
        "  #mask = np.logical_and(mask, np.array(ca_exp) <= 67)\n",
        "\n",
        "  \"\"\"ca_exp = np.array(ca_exp)[mask]\n",
        "  intensity = np.array(intensity)[mask]\n",
        "  inadequate = np.array(inadequate)[mask]\"\"\"\n",
        "\n",
        "  #print(sum(ca_exp*intensity)/sum(intensity))\n",
        "  #print(sum(inadequate*intensity)/sum(intensity))\n",
        "  #print(sum(x)/len(x))\n",
        "  #print(sum(j)/len(j))\n",
        "\n",
        "  atom_key={\n",
        "            \"ca\":0,\n",
        "            \"cb\":1,\n",
        "            \"co\":2,\n",
        "            \"n\":3,\n",
        "          }\n",
        "\n",
        "  nonCys_nonGly_aas={\"A\", \"D\", \"E\", \"F\", \"H\", \"I\", \"K\", \"L\", \"M\",\n",
        "                    \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"Y\", \"W\"}\n",
        "\n",
        "  nonCys_aas={\"A\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \"M\",\n",
        "              \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"Y\", \"W\"}\n",
        "\n",
        "  na=np.nan\n",
        "\n",
        "  aa_spec_ranges={ #ranges of interest for chemical shifts\n",
        "                  #aa1 Ca      Cb      C         N\n",
        "                  # Ca & Cb adjusted to 17ppm to match Fig5\n",
        "                  'A':[[15,25],[55,89],[47,57],[170,184],[112,133]],\n",
        "                  'R':[[47,64],[22,39],[169,183],[112,133]],\n",
        "                  'N':[[44,61],[30,47],[168,182],[106,127]],\n",
        "                  'D':[[45,62],[33,50],[169,183],[109,130]],\n",
        "  #               'C':[[50,67],[22,39],[168,182],[105,126]],\n",
        "                  'Q':[[47,64],[22,39],[169,183],[108,129]],\n",
        "                  'E':[[49,66],[22,39],[169,183],[109,130]],\n",
        "                  #'G':[[36,53],[203,234],[167,181],[ 98,119]],\n",
        "                  'G':[[40,50],[203,234],[169,179],[ 98,119]],\n",
        "                  'H':[[49,66],[23,40],[168,182],[107,128]],\n",
        "                  'I':[[54,71],[31,48],[168,182],[109,130]],\n",
        "                  'L':[[45,62],[33,50],[170,184],[110,131]],\n",
        "                  'K':[[49,66],[26,43],[169,183],[109,130]],\n",
        "                  'M':[[47,64],[26,43],[169,183],[108,129]],\n",
        "                  'F':[[50,67],[31,48],[168,182],[107,128]],\n",
        "                  'P':[[57,74],[25,42],[170,184],[ na, na]],\n",
        "                  'S':[[50,67],[105,139],[55,72],[167,181],[104,125]],\n",
        "                  'T':[[54,71],[114,148],[60,77],[168,182],[102,123]],\n",
        "                  'W':[[48,65],[21,38],[169,183],[109,130]],\n",
        "                  'Y':[[50,67],[30,47],[168,182],[108,129]],\n",
        "                  'V':[[54,71],[25,42],[169,183],[108,129]],\n",
        "                  }\n",
        "\n",
        "\n",
        "  aa_atom_groups={\n",
        "                  'A':[['co','ca'],['ca','cb']],\n",
        "  #               'C':[['co','ca'],['ca','cb']],\n",
        "                  'D':[['co','ca'],['ca','cb']],\n",
        "                  'E':[['co','ca'],['ca','cb']],\n",
        "                  'F':[['co','ca'],['ca','cb']],\n",
        "                  'G':[['co','ca']],\n",
        "                  'H':[['co','ca'],['ca','cb']],\n",
        "                  'I':[['co','ca'],['ca','cb']],\n",
        "                  'K':[['co','ca'],['ca','cb']],\n",
        "                  'L':[['co','ca'],['ca','cb']],\n",
        "                  'M':[['co','ca'],['ca','cb']],\n",
        "                  'N':[['co','ca'],['ca','cb']],\n",
        "                  'P':[['co','ca'],['ca','cb']],\n",
        "                  'Q':[['co','ca'],['ca','cb']],\n",
        "                  'R':[['co','ca'],['ca','cb']],\n",
        "                  'S':[['co','ca'],['ca','cb']],\n",
        "                  'T':[['co','ca'],['ca','cb']],\n",
        "                  'V':[['co','ca'],['ca','cb']],\n",
        "                  'Y':[['co','ca'],['ca','cb']],\n",
        "                  'W':[['co','ca'],['ca','cb']],\n",
        "                }\n",
        "\n",
        "  atom_a = \"ca\" #defining atom_a as the alpha carbon\n",
        "  atom_b = \"cb\" #defining atom_b as the beta carbon\n",
        "  aaX = aa1 #define aaX as residue of interest (one letter code)\n",
        "  points = zip(atom_a, atom_b) #create an array with both atom_a and atom_b\n",
        "  #above_diag = (filter(lambda i: i[0] > i[1], points)) #select only points above the diagonal due to mirroring\n",
        "\n",
        "\n",
        "                            # figsize is in inches, modify for different size\n",
        "  aaX = aa\n",
        "  subplot_counter = 0\n",
        "  #for subplot_counter, input_file in enumerate(input_file_paths):\n",
        "      # also, extract amino acid information #\n",
        "      # get the first letter of the input file - modify manually if necessary #\n",
        "  aaX = aa1 #amino acid - easiest to modify manually\n",
        "    #input_file.split(\"/\")[-1][0] # modify this to get aa1 info\n",
        "  ###################################\n",
        "      ### AUTOMATIC PART FROM HERE ON ###\n",
        "      # skip G and P when the specified atom/CS is not there\n",
        "  atom_a = atom_a.lower(); atom_b = atom_b.lower()#\n",
        "\n",
        "  CAs=[]; CBs=[]; COs=[]; Ns=[]\n",
        "\n",
        "  # join CS of each atom into one list #\n",
        "  if aa1 in nonCys_nonGly_aas:\n",
        "          all_CS = [x, y, y, z]\n",
        "  elif aa1 == 'G':\n",
        "          all_CS = [x, j, y, w]\n",
        "  elif aa1 == 'C':\n",
        "          print('Skipping cysteine since PPM has no prediction for Cys.')\n",
        "  else:\n",
        "          print('Error: amino acid \\\"{}\\\" not implemented'.format(aa1))\n",
        "          sys.exit()\n",
        "\n",
        "        # set atom selection so that lower average(ppm) is first (x-axis) ...\n",
        "        # ... which draws above peaks above the diagonal. Higher avgerage(ppm) ...\n",
        "        # ... draws the peaks as peaks below the diagonal.\n",
        "\n",
        "\n",
        "\n",
        "  x_key=atom_key[atom_a]\n",
        "  y_key=atom_key[atom_b]\n",
        "  #w = [1.198e+04,3.702e+04,3.702e+04,-5.846e+05,5.808e+05,-4.077e+04]\n",
        "  bin_width=0.25\n",
        "  mtick_spacing=5\n",
        "  x_lims = np.array(aa_spec_ranges[aa1][0])\n",
        "  y_lims = np.array(aa_spec_ranges[aa1][2])\n",
        "  x_edges = np.arange(x_lims[0],x_lims[1]+bin_width,bin_width) #create an array from one limit to the next using bin widths\n",
        "  y_edges=np.arange(y_lims[0],y_lims[1]+bin_width,bin_width)\n",
        "  hw=bin_width/2\n",
        "  center_bins_x=np.arange(x_lims[0]+hw,x_lims[1]+hw,bin_width)\n",
        "  center_bins_y=np.arange(y_lims[0]+hw,y_lims[1]+hw,bin_width)\n",
        "\n",
        "\n",
        "        # calculate counts for bins #\n",
        "  H1, npxedges, npyedges = np.histogram2d(y, x,\n",
        "                                          bins=(y_edges,x_edges))#, weights = [w[i]]*len(all_CS[y_key]),)\n",
        "\n",
        "  #H2, npxedges, npyedges = np.histogram2d(all_CS[y_key],all_CS[2],bins=(y_edges,x_edges))#, weights = [w[i]]*len(all_CS[y_key]),)\n",
        "        # set contouring parameters #\n",
        "  #H = np.sum((H1,H2), axis=0)\n",
        "  H = H1\n",
        "\n",
        "  np.set_printoptions(threshold=np.inf)\n",
        "\n",
        "        # set contouring parameters #\n",
        "  h_lists = []\n",
        "  h = []\n",
        "\n",
        "  fwhm = 3 # Full width at half maximum\n",
        "  def gaussian2d(x, y, amplitude, xo, yo, fwhm):\n",
        "    d = fwhm / (2 * np.sqrt(2 * np.log(2)))\n",
        "    h_list = [0]*len(H)\n",
        "    xgrid, ygrid = np.meshgrid(x, y)\n",
        "    g = np.zeros_like(xgrid)\n",
        "    for i in range(len(xo)-1):\n",
        "        for j in range(len(yo)-1):\n",
        "            a = 1/(2*d**2)\n",
        "            c = 1/(2*d**2)\n",
        "            xi = np.linspace(xo[i] - 3 * d, xo[i] + 3 * d, len(xo))\n",
        "            yi = np.linspace(yo[j] - 3 * d, yo[j] + 3 * d, len(yo))\n",
        "            xigrid, yigrid = np.meshgrid(xi, yi)\n",
        "            gi = amplitude[i][j]*np.exp( - (a*((xigrid-xo[i])**2) + a*((yigrid-yo[j])**2)))\n",
        "            H3, npxedges1, npyedges1 = np.histogram2d(yigrid.ravel(),xigrid.ravel(),bins=(npyedges,npxedges),weights = gi.ravel())\n",
        "            h_list.append(H3)\n",
        "            H3, npxedges1, npyedges1 = np.histogram2d(yigrid.ravel(),xigrid.ravel(),bins=(npyedges,npxedges),weights = gi.ravel())\n",
        "            h_list.append(H3)\n",
        "    h_lists.append(sum(h_list))\n",
        "    h = sum(h_lists)\n",
        "    return(h)\n",
        "  Z = gaussian2d(npxedges[:-1], npyedges[:-1], H, npxedges[:-1], npyedges[:-1], fwhm)\n",
        "\n",
        "\n",
        "  adjDistX = .125\n",
        "  adjDistY = .125\n",
        "  adjDistY /= 2\n",
        "  new_x_edges = np.arange(x_lims[0] - adjDistX, x_lims[1] - adjDistX + bin_width,bin_width)\n",
        "  #x_edges = np.arange(x_lims[0][0], x_lims[0][1] + bin_width,bin_width)\n",
        "  new_y_edges=np.arange(y_lims[0] - adjDistY * 2 ,y_lims[1] - adjDistY * 2 + bin_width,bin_width)\n",
        "  ###\n",
        "\n",
        "  x_edges = new_x_edges\n",
        "  y_edges = new_y_edges\n",
        "\n",
        "  H_exp, npxedges_exp, npyedges_exp = np.histogram2d(ca_exp, inadequate, bins=(y_edges,x_edges), weights=intensity)\n",
        "\n",
        "  h = Z\n",
        "  c=[]\n",
        "  d =[]\n",
        "  for i in range(len(h)):\n",
        "    c = h[i-1]/sum(sum(h))\n",
        "    d.append(c)\n",
        "  H=np.array(d).T\n",
        "  c=[]\n",
        "  d =[]\n",
        "\n",
        "  #####################################################################\n",
        "  H_exp = sp.ndimage.uniform_filter(H_exp, size=4, mode='constant')\n",
        "  ### Adjusting the spectra measured for the experimental\n",
        "\n",
        "\n",
        "  ###Change this for varying heights for each graph\n",
        "  H_exp_max = max([max(array) for array in H_exp])\n",
        "  H_max = max([max(array) for array in H])\n",
        "\n",
        "  for i in range(len(H_exp)):\n",
        "    #c = (H_exp[i-1])/sum(sum(H_exp))\n",
        "    c = (H_exp[i-1]) * H_max / H_exp_max\n",
        "    #c = (H_exp[i-1]*0.007033550868176652)/(sum(sum(h_exp))*0.0022451602109917292 )\n",
        "    d.append(c)\n",
        "  #####################################################################\n",
        "\n",
        "  H_exp = np.array(d)\n",
        "\n",
        "\n",
        "  h = np.subtract(H_exp,H)\n",
        "  savetxt('urea - GLY',h,delimiter=',')\n",
        "\n",
        "\n",
        "  max_cont = np.max(H)\n",
        "  #ensemble_size = len(all_CS[y_key])\n",
        "  #factor size of 1.15\n",
        "  #13steps\n",
        "  min_cont = np.min(H) # ensemble_size/2000*(5units*bin_width)\n",
        "  step = []\n",
        "  for i in range(13):\n",
        "    x = 1.15**i\n",
        "    step.append(x)\n",
        "  steps = sum(step)\n",
        "  step_cont = np.subtract(max_cont,min_cont)/steps\n",
        "  array = np.zeros(13)\n",
        "  for i in range(13):\n",
        "    if i == 0:\n",
        "      array[i] = (((1.15 ** i) * (step_cont))) + min_cont\n",
        "    else:\n",
        "      array[i] = (((1.15 ** i) * (step_cont))) + array[i-1]\n",
        "  levels = array\n",
        "  \"\"\"min_cont = np.min(h) # ensemble_size/2000*(5units*bin_width)\n",
        "  max_cont = np.max(h)\n",
        "\n",
        "\n",
        "  step = []\n",
        "  for i in range(13):\n",
        "    x = 1.15**i\n",
        "    step.append(x)\n",
        "  steps = sum(step)\n",
        "  step_cont = np.subtract(max_cont,min_cont)/steps\n",
        "  array = np.zeros(13)\n",
        "  for i in range(13):\n",
        "    if i == 0:\n",
        "      array[i] = (((1.15 ** i) * (step_cont))) + min_cont\n",
        "    else:\n",
        "      array[i] = (((1.15 ** i) * (step_cont))) + array[i-1]\n",
        "\n",
        "  levels = array\"\"\"\n",
        "        # draw the subplot #\n",
        "  fig  = plt.figure(figsize=(30,10))\n",
        "\n",
        "  new_x_edges = x_edges\n",
        "  new_y_edges = y_edges\n",
        "\n",
        "  xedges = new_x_edges\n",
        "  yedges = new_y_edges\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  ax1 = fig.add_subplot(131, projection = '3d')\n",
        "  ax2 = fig.add_subplot(132, projection = '3d')\n",
        "  ax3 = fig.add_subplot(133, projection = '3d')\n",
        "  axs = [ax1, ax2, ax3]\n",
        "\n",
        "  HS = [H_exp, H, np.subtract(H_exp,H)]\n",
        "\n",
        "  xpos, ypos = np.meshgrid(xedges[:-1]+xedges[1:], yedges[:-1]+yedges[1:])\n",
        "\n",
        "  xpos = xpos.flatten()/2.\n",
        "  ypos = ypos.flatten()/2.\n",
        "  zpos = np.zeros_like(xpos)\n",
        "\n",
        "  dx = xedges [1] - xedges [0]\n",
        "  dy = yedges [1] - yedges [0]\n",
        "\n",
        "  tempZ = np.array(HS).flatten().flatten()\n",
        "\n",
        "  max_height = np.max(tempZ)   # get range of colorbars so we can normalize\n",
        "  min_height = -1 * np.max(tempZ)\n",
        "\n",
        "  for i in range(len(HS)):\n",
        "    dz = HS[i].flatten()\n",
        "\n",
        "    cmap = cm.RdBu # Get desired colormap - you can change this!\n",
        "\n",
        "    # scale each z to [0,1], and get their rgb values\n",
        "    rgba = [cmap((k-min_height)/(max_height*2)) for k in dz]\n",
        "\n",
        "    axs[i].bar3d(xpos, ypos, zpos, dx, dy, dz, color=rgba, zsort='average')\n",
        "    #axs[i].set_xlim(40, 50)\n",
        "    #axs[i].set_ylim(169, 179)\n",
        "    axs[i].set_zlim(-.5*max_height, max_height*1.1)\n",
        "    axs[i].w_zaxis.line.set_lw(0.)\n",
        "    axs[i].set_zticks([])\n",
        "    axs[i].view_init(elev=0, azim = 45)\n",
        "    axs[i].grid(False)\n",
        "    axs[i].set_facecolor('white')\n",
        "    #axs[i].axis(\"off\")\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  ax1 = fig.add_subplot(131)\n",
        "  ax2 = fig.add_subplot(132)\n",
        "  ax3 = fig.add_subplot(133)\n",
        "  axs = [ax1, ax2, ax3]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  a = axs[0].contour(x_edges[:-1], y_edges[:-1], H_exp, levels = levels, cmap='RdBu',linewidths=0.5)\n",
        "  b = axs[1].contour(x_edges[:-1], y_edges[:-1], H, levels = levels, cmap='RdBu',linewidths=0.5)\n",
        "\n",
        "  min_cont = np.min(H) # ensemble_size/2000*(5units*bin_width)\n",
        "  max_cont = np.max(H) ###Controls the contour lines\n",
        "\n",
        "  c = axs[2].contour(x_edges[:-1], y_edges[:-1], h, levels = levels, cmap='RdBu',linewidths=0.5)\n",
        "\n",
        "  for n in range(3):\n",
        "    axs[n].set_aspect('equal')\n",
        "    #axs[n].set_xlim(x_lims[0],x_lims[1])\n",
        "    #axs[n].set_ylim(y_lims[0],y_lims[1])\n",
        "    axs[n].xaxis.set_major_locator(pltticker.MultipleLocator(mtick_spacing))\n",
        "    axs[n].yaxis.set_major_locator(pltticker.MultipleLocator(mtick_spacing))\n",
        "    axs[n].xaxis.set_minor_locator(pltticker.MultipleLocator(1))\n",
        "    axs[n].yaxis.set_minor_locator(pltticker.MultipleLocator(1))\n",
        "    axs[n].set_xlabel('ppm', labelpad=1.0)\n",
        "    axs[n].set_ylabel('ppm', labelpad=1.0)\n",
        "    axs[n].invert_xaxis()\n",
        "    axs[n].invert_yaxis()\n",
        "\n",
        "  #\"\"\"\n",
        "\n",
        "  plt.savefig(\"ALA_SIDE_3D_E.png\", format = \"png\", dpi = 300)\n",
        "  #plt.savefig(\"230721_ALA_unfiltered.svg\", format = \"svg\")\n",
        "\n",
        "PDB_file = \"A_10K.out\"\n",
        "exp_file = \"230319_VGLASTasynA30_mono_sf_DARR_INADEQUATE_750uM_made230223_8Murea-2D_ALA_AD.txt\"\n",
        "output_file_path = \"out\"\n",
        "aa = \"A\"\n",
        "\n",
        "main(PDB_file, exp_file, output_file_path, aa)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#No edits\n"
      ],
      "metadata": {
        "id": "hQzckJLVC7E9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from numpy import savetxt\n",
        "def main(PDB_file, exp_file, output_file_path, aa):\n",
        "  aa1 = aa\n",
        "  data = []\n",
        "  with open(PDB_file, mode=\"r\") as infile: #open data file and read contents\n",
        "    reader = csv.reader(infile, delimiter=\"\\n\")\n",
        "    for row in reader:\n",
        "      if len(row) == 1:\n",
        "        data.append(re.split(r'\\s+', row[0])[2:])\n",
        "      else:\n",
        "        data.append(row)\n",
        "  def get_row_floats(data, row):\n",
        "    return list(map(lambda x: float(x[row]), data))\n",
        "    #print(f'Data: {data}\\n\\n')\n",
        "\n",
        "  phi = get_row_floats(data[1:], 0) #def z as CO shift\n",
        "  psi = get_row_floats(data[1:], 1) #def w as N shift\n",
        "  x = get_row_floats(data[1:], 3)\n",
        "  y = get_row_floats(data[1:], 2) ######Switch\n",
        "  z = get_row_floats(data[1:], 4)\n",
        "  w = get_row_floats(data[1:], 5)\n",
        "  j = []\n",
        "  j = y\n",
        "  data = []\n",
        "  count = 0\n",
        "\n",
        "  with open(exp_file, mode=\"r\") as infile: #open data file and read contents\n",
        "    chunk_size = 1000\n",
        "    while True:\n",
        "        chunk = infile.readlines(chunk_size)\n",
        "        if not chunk:\n",
        "          break\n",
        "        for line in chunk:\n",
        "          reader = csv.reader(infile)\n",
        "          for row in reader:\n",
        "            count += 1\n",
        "            if len(row) == 1:\n",
        "              data.append(re.split(r'\\s+', row[0])[1:])\n",
        "            else:\n",
        "              data.append(row)\n",
        "\n",
        "  def get_row_floats(data, row):\n",
        "    return list(map(lambda x: float(x[row]), data))\n",
        "\n",
        "\n",
        "  ca_exp = get_row_floats(data, 0) #def x as Ca shift\n",
        "  inadequate = get_row_floats(data, 1)\n",
        "  cb_exp = get_row_floats(data, 1) #def y as Cb shift\n",
        "  intensity = get_row_floats(data, 2)\n",
        "\n",
        "\n",
        "  for i in range(len(inadequate)):\n",
        "    inadequate[i-1] = inadequate[i-1]-1\n",
        "  min = np.min(intensity)\n",
        "  for i in range(len(intensity)):\n",
        "    intensity[i-1] = intensity[i-1]-min\n",
        "\n",
        "  mask = np.logical_and(np.array(inadequate) >= 29, np.array(inadequate) <= 37)\n",
        "  mask = np.logical_and(mask, np.array(ca_exp) >= 54)\n",
        "  mask = np.logical_and(mask, np.array(ca_exp) <= 71)\n",
        "  #mask = np.logical_and(mask, np.array(intensity) >= 0)\n",
        "\n",
        "  #mask = np.logical_and(np.array(inadequate) >= 105, np.array(inadequate) <= 139)\n",
        "  #mask = np.logical_and(mask, np.array(ca_exp) >= 50)\n",
        "  #mask = np.logical_and(mask, np.array(ca_exp) <= 67)\n",
        "\n",
        "  \"\"\"ca_exp = np.array(ca_exp)[mask]\n",
        "  intensity = np.array(intensity)[mask]\n",
        "  inadequate = np.array(inadequate)[mask]\"\"\"\n",
        "\n",
        "  #print(sum(ca_exp*intensity)/sum(intensity))\n",
        "  #print(sum(inadequate*intensity)/sum(intensity))\n",
        "  #print(sum(x)/len(x))\n",
        "  #print(sum(j)/len(j))\n",
        "\n",
        "  atom_key={\n",
        "            \"ca\":0,\n",
        "            \"cb\":1,\n",
        "            \"co\":2,\n",
        "            \"n\":3,\n",
        "          }\n",
        "\n",
        "  nonCys_nonGly_aas={\"A\", \"D\", \"E\", \"F\", \"H\", \"I\", \"K\", \"L\", \"M\",\n",
        "                    \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"Y\", \"W\"}\n",
        "\n",
        "  nonCys_aas={\"A\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"K\", \"L\", \"M\",\n",
        "              \"N\", \"P\", \"Q\", \"R\", \"S\", \"T\", \"V\", \"Y\", \"W\"}\n",
        "\n",
        "  na=np.nan\n",
        "\n",
        "  aa_spec_ranges={ #ranges of interest for chemical shifts\n",
        "                  #aa1 Ca      Cb      C         N\n",
        "                  # Ca & Cb adjusted to 17ppm to match Fig5\n",
        "                  'A':[[15,25],[55,89],[47,57],[170,184],[112,133]],\n",
        "                  'R':[[47,64],[22,39],[169,183],[112,133]],\n",
        "                  'N':[[44,61],[30,47],[168,182],[106,127]],\n",
        "                  'D':[[45,62],[33,50],[169,183],[109,130]],\n",
        "  #               'C':[[50,67],[22,39],[168,182],[105,126]],\n",
        "                  'Q':[[47,64],[22,39],[169,183],[108,129]],\n",
        "                  'E':[[49,66],[22,39],[169,183],[109,130]],\n",
        "                  #'G':[[36,53],[203,234],[167,181],[ 98,119]],\n",
        "                  'G':[[40,50],[203,234],[169,179],[ 98,119]],\n",
        "                  'H':[[49,66],[23,40],[168,182],[107,128]],\n",
        "                  'I':[[54,71],[31,48],[168,182],[109,130]],\n",
        "                  'L':[[45,62],[33,50],[170,184],[110,131]],\n",
        "                  'K':[[49,66],[26,43],[169,183],[109,130]],\n",
        "                  'M':[[47,64],[26,43],[169,183],[108,129]],\n",
        "                  'F':[[50,67],[31,48],[168,182],[107,128]],\n",
        "                  'P':[[57,74],[25,42],[170,184],[ na, na]],\n",
        "                  'S':[[50,67],[105,139],[55,72],[167,181],[104,125]],\n",
        "                  'T':[[54,71],[114,148],[60,77],[168,182],[102,123]],\n",
        "                  'W':[[48,65],[21,38],[169,183],[109,130]],\n",
        "                  'Y':[[50,67],[30,47],[168,182],[108,129]],\n",
        "                  'V':[[54,71],[25,42],[169,183],[108,129]],\n",
        "                  }\n",
        "\n",
        "\n",
        "  aa_atom_groups={\n",
        "                  'A':[['co','ca'],['ca','cb']],\n",
        "  #               'C':[['co','ca'],['ca','cb']],\n",
        "                  'D':[['co','ca'],['ca','cb']],\n",
        "                  'E':[['co','ca'],['ca','cb']],\n",
        "                  'F':[['co','ca'],['ca','cb']],\n",
        "                  'G':[['co','ca']],\n",
        "                  'H':[['co','ca'],['ca','cb']],\n",
        "                  'I':[['co','ca'],['ca','cb']],\n",
        "                  'K':[['co','ca'],['ca','cb']],\n",
        "                  'L':[['co','ca'],['ca','cb']],\n",
        "                  'M':[['co','ca'],['ca','cb']],\n",
        "                  'N':[['co','ca'],['ca','cb']],\n",
        "                  'P':[['co','ca'],['ca','cb']],\n",
        "                  'Q':[['co','ca'],['ca','cb']],\n",
        "                  'R':[['co','ca'],['ca','cb']],\n",
        "                  'S':[['co','ca'],['ca','cb']],\n",
        "                  'T':[['co','ca'],['ca','cb']],\n",
        "                  'V':[['co','ca'],['ca','cb']],\n",
        "                  'Y':[['co','ca'],['ca','cb']],\n",
        "                  'W':[['co','ca'],['ca','cb']],\n",
        "                }\n",
        "\n",
        "  atom_a = \"ca\" #defining atom_a as the alpha carbon\n",
        "  atom_b = \"cb\" #defining atom_b as the beta carbon\n",
        "  aaX = aa1 #define aaX as residue of interest (one letter code)\n",
        "  points = zip(atom_a, atom_b) #create an array with both atom_a and atom_b\n",
        "  #above_diag = (filter(lambda i: i[0] > i[1], points)) #select only points above the diagonal due to mirroring\n",
        "\n",
        "\n",
        "                            # figsize is in inches, modify for different size\n",
        "  aaX = aa\n",
        "  subplot_counter = 0\n",
        "  #for subplot_counter, input_file in enumerate(input_file_paths):\n",
        "      # also, extract amino acid information #\n",
        "      # get the first letter of the input file - modify manually if necessary #\n",
        "  aaX = aa1 #amino acid - easiest to modify manually\n",
        "    #input_file.split(\"/\")[-1][0] # modify this to get aa1 info\n",
        "  ###################################\n",
        "      ### AUTOMATIC PART FROM HERE ON ###\n",
        "      # skip G and P when the specified atom/CS is not there\n",
        "  atom_a = atom_a.lower(); atom_b = atom_b.lower()#\n",
        "\n",
        "  CAs=[]; CBs=[]; COs=[]; Ns=[]\n",
        "\n",
        "  # join CS of each atom into one list #\n",
        "  if aa1 in nonCys_nonGly_aas:\n",
        "          all_CS = [x, y, y, z]\n",
        "  elif aa1 == 'G':\n",
        "          all_CS = [x, j, y, w]\n",
        "  elif aa1 == 'C':\n",
        "          print('Skipping cysteine since PPM has no prediction for Cys.')\n",
        "  else:\n",
        "          print('Error: amino acid \\\"{}\\\" not implemented'.format(aa1))\n",
        "          sys.exit()\n",
        "\n",
        "        # set atom selection so that lower average(ppm) is first (x-axis) ...\n",
        "        # ... which draws above peaks above the diagonal. Higher avgerage(ppm) ...\n",
        "        # ... draws the peaks as peaks below the diagonal.\n",
        "\n",
        "\n",
        "\n",
        "  x_key=atom_key[atom_a]\n",
        "  y_key=atom_key[atom_b]\n",
        "  #w = [1.198e+04,3.702e+04,3.702e+04,-5.846e+05,5.808e+05,-4.077e+04]\n",
        "  bin_width=0.25\n",
        "  mtick_spacing=5\n",
        "  x_lims = np.array(aa_spec_ranges[aa1][0])\n",
        "  y_lims = np.array(aa_spec_ranges[aa1][2])\n",
        "  x_edges = np.arange(x_lims[0],x_lims[1]+bin_width,bin_width) #create an array from one limit to the next using bin widths\n",
        "  y_edges=np.arange(y_lims[0],y_lims[1]+bin_width,bin_width)\n",
        "  hw=bin_width/2\n",
        "  center_bins_x=np.arange(x_lims[0]+hw,x_lims[1]+hw,bin_width)\n",
        "  center_bins_y=np.arange(y_lims[0]+hw,y_lims[1]+hw,bin_width)\n",
        "\n",
        "\n",
        "        # calculate counts for bins #\n",
        "  H1, npxedges, npyedges = np.histogram2d(y, x,\n",
        "                                          bins=(y_edges,x_edges))#, weights = [w[i]]*len(all_CS[y_key]),)\n",
        "\n",
        "  #H2, npxedges, npyedges = np.histogram2d(all_CS[y_key],all_CS[2],bins=(y_edges,x_edges))#, weights = [w[i]]*len(all_CS[y_key]),)\n",
        "        # set contouring parameters #\n",
        "  #H = np.sum((H1,H2), axis=0)\n",
        "  H = H1\n",
        "\n",
        "  np.set_printoptions(threshold=np.inf)\n",
        "\n",
        "        # set contouring parameters #\n",
        "  h_lists = []\n",
        "  h = []\n",
        "\n",
        "  fwhm = 1.5 # Full width at half maximum\n",
        "  def gaussian2d(x, y, amplitude, xo, yo, fwhm):\n",
        "    d = fwhm / (2 * np.sqrt(2 * np.log(2)))\n",
        "    h_list = [0]*len(H)\n",
        "    xgrid, ygrid = np.meshgrid(x, y)\n",
        "    g = np.zeros_like(xgrid)\n",
        "    for i in range(len(xo)-1):\n",
        "        for j in range(len(yo)-1):\n",
        "            a = 1/(2*d**2)\n",
        "            c = 1/(2*d**2)\n",
        "            xi = np.linspace(xo[i] - 3 * d, xo[i] + 3 * d, len(xo))\n",
        "            yi = np.linspace(yo[j] - 3 * d, yo[j] + 3 * d, len(yo))\n",
        "            xigrid, yigrid = np.meshgrid(xi, yi)\n",
        "            gi = amplitude[i][j]*np.exp( - (a*((xigrid-xo[i])**2) + a*((yigrid-yo[j])**2)))\n",
        "            H3, npxedges1, npyedges1 = np.histogram2d(yigrid.ravel(),xigrid.ravel(),bins=(npyedges,npxedges),weights = gi.ravel())\n",
        "            h_list.append(H3)\n",
        "            H3, npxedges1, npyedges1 = np.histogram2d(yigrid.ravel(),xigrid.ravel(),bins=(npyedges,npxedges),weights = gi.ravel())\n",
        "            h_list.append(H3)\n",
        "    h_lists.append(sum(h_list))\n",
        "    h = sum(h_lists)\n",
        "    return(h)\n",
        "  Z = gaussian2d(npxedges[:-1], npyedges[:-1], H, npxedges[:-1], npyedges[:-1], fwhm)\n",
        "\n",
        "\n",
        "  adjDistX = 0\n",
        "  adjDistY = 0\n",
        "  adjDistY /= 2\n",
        "  new_x_edges = np.arange(x_lims[0] - adjDistX, x_lims[1] - adjDistX + bin_width,bin_width)\n",
        "  #x_edges = np.arange(x_lims[0][0], x_lims[0][1] + bin_width,bin_width)\n",
        "  new_y_edges=np.arange(y_lims[0] - adjDistY * 2 ,y_lims[1] - adjDistY * 2 + bin_width,bin_width)\n",
        "  ###\n",
        "\n",
        "  x_edges = new_x_edges\n",
        "  y_edges = new_y_edges\n",
        "\n",
        "  H_exp, npxedges_exp, npyedges_exp = np.histogram2d(ca_exp, inadequate, bins=(y_edges,x_edges), weights=intensity)\n",
        "\n",
        "  h = Z\n",
        "  c=[]\n",
        "  d =[]\n",
        "  for i in range(len(h)):\n",
        "    c = h[i-1]/sum(sum(h))\n",
        "    d.append(c)\n",
        "  H=np.array(d).T\n",
        "  c=[]\n",
        "  d =[]\n",
        "\n",
        "  #####################################################################\n",
        "  #H_exp = sp.ndimage.uniform_filter(H_exp, size=4, mode='constant')\n",
        "  ### Adjusting the spectra measured for the experimental\n",
        "\n",
        "\n",
        "  ###Change this for varying heights for each graph\n",
        "  H_exp_max = max([max(array) for array in H_exp])\n",
        "  H_max = max([max(array) for array in H])\n",
        "\n",
        "  for i in range(len(H_exp)):\n",
        "    #c = (H_exp[i-1])/sum(sum(H_exp))\n",
        "    c = (H_exp[i-1]) * H_max / H_exp_max\n",
        "    #c = (H_exp[i-1]*0.007033550868176652)/(sum(sum(h_exp))*0.0022451602109917292 )\n",
        "    d.append(c)\n",
        "  #####################################################################\n",
        "\n",
        "  H_exp = np.array(d)\n",
        "\n",
        "\n",
        "  h = np.subtract(H_exp,H)\n",
        "  savetxt('urea - GLY',h,delimiter=',')\n",
        "\n",
        "\n",
        "  max_cont = np.max(H)\n",
        "  #ensemble_size = len(all_CS[y_key])\n",
        "  #factor size of 1.15\n",
        "  #13steps\n",
        "  min_cont = np.min(H) # ensemble_size/2000*(5units*bin_width)\n",
        "  step = []\n",
        "  for i in range(13):\n",
        "    x = 1.15**i\n",
        "    step.append(x)\n",
        "  steps = sum(step)\n",
        "  step_cont = np.subtract(max_cont,min_cont)/steps\n",
        "  array = np.zeros(13)\n",
        "  for i in range(13):\n",
        "    if i == 0:\n",
        "      array[i] = (((1.15 ** i) * (step_cont))) + min_cont\n",
        "    else:\n",
        "      array[i] = (((1.15 ** i) * (step_cont))) + array[i-1]\n",
        "  levels = array\n",
        "  \"\"\"min_cont = np.min(h) # ensemble_size/2000*(5units*bin_width)\n",
        "  max_cont = np.max(h)\n",
        "\n",
        "\n",
        "  step = []\n",
        "  for i in range(13):\n",
        "    x = 1.15**i\n",
        "    step.append(x)\n",
        "  steps = sum(step)\n",
        "  step_cont = np.subtract(max_cont,min_cont)/steps\n",
        "  array = np.zeros(13)\n",
        "  for i in range(13):\n",
        "    if i == 0:\n",
        "      array[i] = (((1.15 ** i) * (step_cont))) + min_cont\n",
        "    else:\n",
        "      array[i] = (((1.15 ** i) * (step_cont))) + array[i-1]\n",
        "\n",
        "  levels = array\"\"\"\n",
        "        # draw the subplot #\n",
        "  fig  = plt.figure(figsize=(30,10))\n",
        "\n",
        "  new_x_edges = x_edges\n",
        "  new_y_edges = y_edges\n",
        "\n",
        "  xedges = new_x_edges\n",
        "  yedges = new_y_edges\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  ax1 = fig.add_subplot(131, projection = '3d')\n",
        "  ax2 = fig.add_subplot(132, projection = '3d')\n",
        "  ax3 = fig.add_subplot(133, projection = '3d')\n",
        "  axs = [ax1, ax2, ax3]\n",
        "\n",
        "  HS = [H_exp, H, np.subtract(H_exp,H)]\n",
        "\n",
        "  xpos, ypos = np.meshgrid(xedges[:-1]+xedges[1:], yedges[:-1]+yedges[1:])\n",
        "\n",
        "  xpos = xpos.flatten()/2.\n",
        "  ypos = ypos.flatten()/2.\n",
        "  zpos = np.zeros_like(xpos)\n",
        "\n",
        "  dx = xedges [1] - xedges [0]\n",
        "  dy = yedges [1] - yedges [0]\n",
        "\n",
        "  tempZ = np.array(HS).flatten().flatten()\n",
        "\n",
        "  max_height = np.max(tempZ)   # get range of colorbars so we can normalize\n",
        "  min_height = -1 * np.max(tempZ)\n",
        "\n",
        "  for i in range(len(HS)):\n",
        "    dz = HS[i].flatten()\n",
        "\n",
        "    cmap = cm.RdBu # Get desired colormap - you can change this!\n",
        "\n",
        "    # scale each z to [0,1], and get their rgb values\n",
        "    rgba = [cmap((k-min_height)/(max_height*2)) for k in dz]\n",
        "\n",
        "    axs[i].bar3d(xpos, ypos, zpos, dx, dy, dz, color=rgba, zsort='average')\n",
        "    #axs[i].set_xlim(40, 50)\n",
        "    #axs[i].set_ylim(169, 179)\n",
        "    axs[i].set_zlim(-.5*max_height, max_height*1.1)\n",
        "    axs[i].w_zaxis.line.set_lw(0.)\n",
        "    axs[i].set_zticks([])\n",
        "    axs[i].view_init(elev=0, azim = 45)\n",
        "    axs[i].grid(False)\n",
        "    axs[i].set_facecolor('white')\n",
        "    #axs[i].axis(\"off\")\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  ax1 = fig.add_subplot(131)\n",
        "  ax2 = fig.add_subplot(132)\n",
        "  ax3 = fig.add_subplot(133)\n",
        "  axs = [ax1, ax2, ax3]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  a = axs[0].contour(x_edges[:-1], y_edges[:-1], H_exp, levels = levels, cmap='RdBu',linewidths=0.5)\n",
        "  b = axs[1].contour(x_edges[:-1], y_edges[:-1], H, levels = levels, cmap='RdBu',linewidths=0.5)\n",
        "\n",
        "  min_cont = np.min(H) # ensemble_size/2000*(5units*bin_width)\n",
        "  max_cont = np.max(H) ###Controls the contour lines\n",
        "\n",
        "  c = axs[2].contour(x_edges[:-1], y_edges[:-1], h, levels = levels, cmap='RdBu',linewidths=0.5)\n",
        "\n",
        "  for n in range(3):\n",
        "    axs[n].set_aspect('equal')\n",
        "    #axs[n].set_xlim(x_lims[0],x_lims[1])\n",
        "    #axs[n].set_ylim(y_lims[0],y_lims[1])\n",
        "    axs[n].xaxis.set_major_locator(pltticker.MultipleLocator(mtick_spacing))\n",
        "    axs[n].yaxis.set_major_locator(pltticker.MultipleLocator(mtick_spacing))\n",
        "    axs[n].xaxis.set_minor_locator(pltticker.MultipleLocator(1))\n",
        "    axs[n].yaxis.set_minor_locator(pltticker.MultipleLocator(1))\n",
        "    axs[n].set_xlabel('ppm', labelpad=1.0)\n",
        "    axs[n].set_ylabel('ppm', labelpad=1.0)\n",
        "    axs[n].invert_xaxis()\n",
        "    axs[n].invert_yaxis()\n",
        "\n",
        "  #\"\"\"\n",
        "\n",
        "  plt.savefig(\"ALA_SIDE_3D_UE.png\", format = \"png\", dpi = 300)\n",
        "  #plt.savefig(\"230721_ALA_unfiltered.svg\", format = \"svg\")\n",
        "\n",
        "PDB_file = \"A_10K.out\"\n",
        "exp_file = \"230319_VGLASTasynA30_mono_sf_DARR_INADEQUATE_750uM_made230223_8Murea-2D_ALA_AD.txt\"\n",
        "output_file_path = \"out\"\n",
        "aa = \"A\"\n",
        "\n",
        "main(PDB_file, exp_file, output_file_path, aa)"
      ],
      "metadata": {
        "id": "BrRZlevrbcKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fg8Ope6SDClX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}